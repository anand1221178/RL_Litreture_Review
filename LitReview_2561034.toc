\contentsline {section}{\numberline {1}Introduction}{2}{section.1}%
\contentsline {section}{\numberline {2}Background}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Reinforcement Learning for Continuous Control}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}The Need for Robustness and Fault Tolerance}{3}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Simulation-Based Learning and the Sim-to-Real Gap}{4}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Key Robustness Paradigms: An Overview}{4}{subsection.2.4}%
\contentsline {section}{\numberline {3}Approaches to Learning Robust Control Policies}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Proactive Training for Robustness}{5}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Domain Randomization and Ensemble Training}{5}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Adversarial Training}{6}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Policy Regularization}{6}{subsubsection.3.1.3}%
\contentsline {subsection}{\numberline {3.2}Reactive Adaptation and Recovery Methods}{6}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Repertoire-Based Adaptation}{6}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Direct Reinforcement Learning Adaptation}{6}{subsubsection.3.2.2}%
\contentsline {subsection}{\numberline {3.3}Comparison of Proactive and Reactive Paradigms}{7}{subsection.3.3}%
\contentsline {section}{\numberline {4}Evaluation of Policy Robustness}{7}{section.4}%
\contentsline {subsection}{\numberline {4.1}Direct Performance Metrics}{8}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Measuring Robustness and Generalization}{8}{subsection.4.2}%
\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}%
\contentsline {section}{\numberline {A}Algorithm and Concept Definitions}{10}{appendix.A}%
\contentsline {subsection}{\numberline {A.1}Proximal Policy Optimization (PPO)}{10}{subsection.A.1}%
\contentsline {subsection}{\numberline {A.2}Twin-Delayed Deep Deterministic Policy Gradient (TD3)}{10}{subsection.A.2}%
\contentsline {subsection}{\numberline {A.3}Trust Region Policy Optimization (TRPO)}{10}{subsection.A.3}%
\contentsline {subsection}{\numberline {A.4}Deep Deterministic Policy Gradient (DDPG)}{10}{subsection.A.4}%
\contentsline {subsection}{\numberline {A.5}Smooth Regularized Reinforcement Learning (SRÂ²L)}{11}{subsection.A.5}%
\contentsline {subsection}{\numberline {A.6}Quality-Diversity (QD)}{11}{subsection.A.6}%
\contentsline {subsection}{\numberline {A.7}MAP-Elites (Multi-dimensional Archive of Phenotypic Elites)}{11}{subsection.A.7}%
\contentsline {subsection}{\numberline {A.8}Intelligent Trial and Error (ITE)}{11}{subsection.A.8}%
\contentsline {subsection}{\numberline {A.9}Reset-Free Trial and Error (RTE)}{12}{subsection.A.9}%
\contentsline {subsection}{\numberline {A.10}Monte Carlo Tree Search (MCTS)}{12}{subsection.A.10}%
\contentsline {subsection}{\numberline {A.11}Partially Observable Markov Decision Process (POMDP)}{12}{subsection.A.11}%
\contentsline {subsection}{\numberline {A.12}Fault-Tolerant Control (FTC)}{12}{subsection.A.12}%
\contentsline {subsection}{\numberline {A.13}Soft Actor-Critic (SAC)}{12}{subsection.A.13}%
